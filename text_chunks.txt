['Interleaved image-text data is ubiquitous on the internet, characterized by multiple images interspersed with text. In recent years, there has been a surge of interest in generating interleaved image-text content', 'However, multimodal story generation poses significant challenges due to the complexity of the inputs and the high demands on the quality of the outputs. Firstly, this task necessitates a thorough comprehension of interleaved data, where text is not only abstract and narrative in nature, but also deeply intertwined with complex images. The model must be adept at deciphering the intricate relationships between images and texts to maintain a coherent narrative flow. Secondly, this task requires the generation of not only a plausible text plot, but also visually captivating images that are consistent in characters and styles. The model should be capable of achieving coherence in the generation of both text and visuals, ensuring an engaging storytelling output.', 'Recently, Multimodal Large Language Models (MLLMs)', 'Specifically, following previous work', 'Furthermore, to enable the efficient generation of coherent long stories, we propose a multimodal attention sink mechanism based on window attention', 'Additionally, we introduce a dataset named StoryStream for training and evaluating multimodal story generation. We design an automatic pipeline that leverages MLLMs to obtain a large-scale and high-resolution dataset featuring a sequence of narrative-rich texts and intriguing images, derived from animated videos. StoryStream is four times larger in terms of data volume compared to the existing largest story dataset', 'Story Visualization vs. Multimodal Story Generation The domain of story visualization, initially pioneered by StoryGAN', 'In the rapidly evolving domain of large language models (LLMs)', 'Visual Story Dataset In the landscape of datasets for visual storytelling, various collections have been developed. The VIST', '3 Method', 'Visual Tokenization and De-tokenization The overview of our method is presented in Figure', 'To effectively extend visual stories, our model must comprehend and generate both images and text. Drawing inspiration from recent advancements in generative MLLMs that unify image comprehension and generation', "Story Instruction Tuning In our instruction tuning process for story generation, we sample a random-length subset of a story data point for each iteration. The model is tasked with predicting the next image and the next sentence of the story text. Within MLLM, all images are converted into image features using a pre-trained ViT tokenizer. For the target text tokens, we perform next-token prediction and use Cross Entropy loss to train for this discrete target. For the target image features, the model uses a series of learnable queries as inputs and continuously outputs a series of latent embeddings. We then compute the cosine similarity loss between the MLLM's output and the target image features. During this stage, we fine-tune the SEED-Story model using a LoRA", 'De-tokenizer Adaptation After instruction tuning, the SEED-Story MLLM effectively produces story images with correct semantics but lacks style consistency and details. We attribute this issue to the misalignment between the latent space of the MLLM output and the image features. To address this, we perform de-tokenizer adaptation for style and texture alignment. In this stage, only the SD-XL image de-tokenizer is trained. Conditioned on the MLLM output embeddings, SD-XL is expected to generate images that are pixel-level aligned with the ground truth. The separate training of the de-tokenizer offers two key advantages. First, it avoids optimization conflicts between the LLM and the de-tokenizer. Second, it conserves memory, making the process executable on GPUs with limited memory.', 'Generating long visual stories has substantial potential in various applications, including education and entertainment. However, creating these stories with MLLMs presents significant challenges. Datasets for extended, interleaved stories are not only rare but also impede the training process due  to their complexity. To address this, we have to employ a train-short-test-long approach, training models on shorter narratives and extending to longer generations during inference. Moreover, during inference, generating significantly longer stories than the training data often leads to model degradation, producing lower-quality images, as illustrated in the first row of Figure', 'A simplistic solution for this is to use a sliding window technique, depicted in Figure', 'To enhance long multimodal generation, we revisit the attention maps of MLLMs. After conducting numerous experiments across various models and cases, we analyze the attention maps across different layers and heads. Our analysis reveals that most queries predominantly focus on four types of tokens:', '(1) starting tokens, (2) punctuation tokens, (3) beginning-of-image (BoI) tokens, and (', 'Building on these insights, we propose a new mechanism for extended generation in MLLMs, termed the multimodal attention sink. During generation, we consistently retain the starting tokens and the image tokens adjacent to the BoI and EoI. Although punctuation tokens receive high attention values, their latent value norms are minimal, contributing insignificantly to the final output, so we do not keep them, as noted by', '4 StoryStream Dataset', 'An ideal source for creating a multimodal story generation dataset is cartoon series, which inherently contain rich plots and consistent character portrayals. We selected three cartoon series to construct our dataset and we present the Curious George in the main body of our paper. The process begins with collecting various series, from which we extract keyframes and their associated subtitles', 'Each keyframe is then processed by GPT-4V', "During dataset construction, we discovered that employing the above chain of thought approach not only produces more accurate narrative text but also speeds up the construction process. Unlike directly feeding all images directly to GPT-4, which is limited to 10 images due to API constrains, our approach produces longer stories. We also significantly improve the model's understanding of each image by incorporating detailed descriptions. This enhancement in image comprehension enriches the narrative details, providing a richer story generation reference.", 'George, the small monkey, was on an icy adventure with his friend, the man in the yellow snowsuit.', 'The snowflakes continued to fall around them, creating a beautiful, albeit cold, scene. Despite the cold, the man comforted George, who was showing signs of concern.', 'They stood together against the snowy backdrop, trying to figure out their next move. George and his friend found themselves standing on an icy platform surrounded by water, with large icebergs in the background.', 'George found comfort in the arms of his friend. They stood together, ready to face whatever came next. The man took a moment to rest, seated against the icy backdrop, still wearing his hat and carrying a backpack, ready for their next move.', 'Suddenly, the man had to run through a snowy canyon. It was a treacherous path with ice floes floating on the water and snowflakes falling from the sky.', 'Inside an icy cave, the man held George close. They peered curiously into the snowy landscape, cautious of what might be out there.', 'Fred is holding his things in the room.', 'Wilma is walking across the room.', 'Fred is standing in a room talking while holding a pile of clothes and a hat over one arm.', 'Wilma is standing in a room, talking.', 'Large-scale. Our StoryStream dataset comprises three subsets totaling 257,850 released images. This represents a significant improvement over existing datasets in terms of scale, specific numbers are presented in column 2 of Table', 'Long Sequence. Moreover, our dataset enhances long story comprehension by offering up to 30 images per story point. Within these 30 images, our corresponding texts present a cohesive narrative, effectively conveying the progression and intricacies of extended stories.   Previous story generation approaches primarily utilize diffusion models, focusing on visualizing story images. These models take the previous image and text as input, and then generate only the next image based on the current text prompt. For a fair comparison, we adapt our model to a visualization-only format. For StoryGen', 'For comparative analysis, there are relatively few established methods for generating multimodal stories. To establish a baseline for comparison, we fine-tune the recently developed MM-interleaved model on our dataset. We detail the comparative results in Figure', 'For qualitative presentation, we demonstrate our SEED-story effectively helps users control the story plot. Figure', 'We provide more cases in Figure', 'As George landed, he noticed a bee hovering nearby and reached out to touch it, curious about the small creature with stripes George was right beside a bee, both of them in a grassy field with a building in the background where a small flower grew between the bricks George was having a great time, smiling and extending his hand towards a bee that was flying near him, with the building and greenery behind him under a clear sky He signaled to George, who was standing next to a house, that they had to go, with a lush green landscape and a wooden fence suggesting a peaceful, rural setting George, wearing his yellow hat, looked puzzled but ready for adventure, standing against a simple grassy background with a stone wall partially visible They hopped into a yellow car, George in the back seat, looking curious, with a building on the right and greenery around as they drove off into the distance They drove through a picturesque rural landscape, with a large red barn, a small cottle, lush greenery, and a winding dirt road, all under a clear sky Meanwhile, the man in the yellow hat was in a bit of a pickle, looking worried as he stood in front of a stone wall and a wooden structure, with a small window above him USER: Please continue the story: "After a day brimming with fun, they went back to their cheerful countryside residence, preparing for new adventures." With a magnifying glass and a camera, George and his friend began their adventure, excited to explore the world around them.', 'George was thrilled to capture the beauty of the hills, the trees, and the wooden fence under the calm sky.', 'He took a picture of a quaint house surrounded by lush greenery, noticing small details through his magnifying lens.', 'Next, George captured his friend holding a book, with a vibrant landscape behind them, making the moment last forever.', 'He continued capturing the vibrant farm with his camera, capting the beauty of the farm, the farmhouse, and the lush greenery.', 'George then took a picture of a child holding a chicken, with a chicken coop in the background, capturing a fun moment on earth.', 'Next, George took a picture of four chickens in front of a small house, noticing a hand holding a blue object in the foreground.', "With a smile, George held his camera, ready to capture another moment, excited to capture more of the world's beauty.", 'George then took a picture of his friend smiling, standing beside a bushy tree under the clear sky.', 'USER: Please continue the story: "On sunny day, George and his friend, the man in the yellow hat, decided to explore their farm." George stood next to a duck in front of a barn, surrounded by a grassy field and trees. The sky was blue with a few clouds.', 'A duck stood in the foreground, with a barn and trees in the background. The sky was blue with a small cloud.', 'George, wearing a puzzled expression, stood in front of a rural landscape with a large barn and a small house.', 'The background showed green fields, trees, and a blue sky with a single cloud.', 'George, with a curious expression, stood in front of a house with a barn in the background. The sky was blue with a small cloud.', 'A man in overalls and a hat was holding a bucket, standing next to a pile of dirt. George and a duck were there also, with a small house in the background.', 'George stood in a farm setting, looking at a small puddle. Behind him, a barn and a house were visible, along with greenery and a fence A man in overalls was kneeling beside a pile of dirt, holding a shovel. George and a duck were there also, looking at the house in the background.', 'A duck stood in a grassy area, with a barn and trees in the background.', 'The sky was blue with a small cloud.', 'A figure in overalls was holding a stick, standing next to a small house in a grassy area. There was a large bush on the side.', 'A man in overalls was digging a hole with a shovel in a grassy area. George and a duck were there also, looking on with curiosity. A small house sat in the background.', 'George stood in a grassy area, looking at a small puddle of water. Behind him, a barn and a house were visible, along with a wooden fence and trees.', 'A man in a hat and overalls was holding a stick, standing next to George. Both were looking at a muddy puddle in a grassy area, near a house and a tree.', 'George stood in a farm setting, looking towards a barn and a house. A puddle of water was on the ground in front of him, with greenery and a fence behind.', 'A character dressed in a suit and hat was standing in front of a rural landscape with a barn and a tree.', 'A man in a hat and shirt stood outside, looking surprised or confused. A tree and a building were visible in the background.', "A man in a shirt and hat stood outside, looking surprised. George, the monkey, held onto the man's leg, looking up at him. A barn, a house, and a tree were in the background.", 'George, the cartoon monkey, looked surprised or worried. The background showed a grassy field with a small puddle of water and a tree trunk.', 'A man in a hat and overalls was kneeling in the grass, reaching out to George. A house and a tree were in the background.', 'George, the cartoon monkey, stood in a grassy area, looking curious. Behind him, a small puddle of water and a tree were visible. George appeared surprised or intrigued, with wide eyes and raised eyebrows.', 'A man in a hat and shirt looked surprised, standing in front of a barn and a tree. His mouth was open, and one hand was raised to his head.', 'George, the monkey, stood on a grassy field, looking curiously at a muddy puddle. Near the puddle, a pair of boots and a hat were visible. Behind, a fence and lush trees suggested a rural setting.', 'A man in a hat and shirt stood in front of a barn, looking surprised with one hand on the chest. A tree and grassy hill were visible in the background.', "A man in a hat and shirt looked surprised, standing in front of a barn and a tree. Soon, George and the man in the yellow hat were back in their car, driving through a green tunnel. The car's interior was a comforting sight.", "They drove along a curved road surrounded by grass and trees. The sky was clear, and George's heart was full of excitement.", "Suddenly, George noticed a cat lying on the road. He crouched near the car's front wheel, watching the cat.", 'George stood near the parked car, looking up at the man in the yellow hat. They were surrounded by trees and a blue sky.', 'George, with his big eyes and mischievous smile, stood ready for the next adventure.', 'They reached a colorful park with a slide, a ladder, and a playground.', 'The green bushes and trees added to the playful atmosphere.', 'George and the man in the yellow hat stood beside the parked car in the green landscape. They were ready for their next adventure.', 'Their journey led them to a vibrant landscape with a large slide in the foreground. A slide extended into a pool, inviting them to play.', 'They reached a vibrant roller coaster track set against a blue sky with fluffy white clouds. The ride featured red and yellow accents, adding to its charm.', 'The amusement park was colorful and bustling with visitors. A large Ferris wheel stood in the foreground, with buildings and greenery in the distance.', 'The man in the yellow hat and George stood in front of a roller coaster track, looking concerned. They were unsure of what to do next.', 'George, with his big, curious eyes, looked up at the man in the yellow hat.', 'He was eager to have some fun.', 'Their journey took them to a colorful amusement park with a towering roller coaster. The park was surrounded by greenery and a cityscape.', 'The next day, George and the man in the yellow hat found themselves on a roller coaster. George appeared excited while the man looked concerned. Their adventure had just begun.', "On their way home, they took a boat ride. The man held onto the steering wheel, while George and a child peeked out from the boat's window, smiling. They were having a great time.", 'Finally, they returned to their colorful house. The walls and the roof under the clear sky made the house look inviting and peaceful.', 'It was the perfect end to a perfect day.', 'The man in the yellow hat and George waved goodbye from the doorway of their house. They were ready for another day of adventures.', 'At the end of the day, they returned to their vibrant yellow house. The walls and roof glowed in the twilight, and the trees and hills in the background added to the peacefulness of their home.', 'Finally, George and the man in the yellow hat sat on a bench inside a colorful cockpit with a steering wheel and control levers. They were ready to go home, but their adventurous spirit was ready for the next day.', 'The man in the yellow hat and George waved goodbye from the doorway of their house. They were ready for another day of adventures in the park, near the river.', 'Their last ride before heading home was a boat. The man held the steering wheel, while George and a child peeked out from a window. They were all smiling, looking forward to their next adventure.', 'Finally, George and the man in the yellow hat sat on a bench inside a colorful cage with a bright wheel. The man looked concerned while George appeared curious. They were ready to go home, but their adventurous spirit was ready for the next day.', 'Their last ride before heading home was a boat. The man held the steering wheel, while George and a child peeked out from the window. They were all smiling, looking forward to their next adventure.', 'The man in the yellow hat smiled as he looked at George, who was standing beside the car. The grassy hills and trees in the background added to the beauty of the scene.', 'To verify our effectiveness in long story generation, we conduct an experiment visualizing a long story using the SEED-Story model, but with varying attention mechanisms. We chunk our data into stories of length of 10 considering the training efficiency. We set the window size as the same as the training length. Qualitative results depict in Figure', 'In terms of efficiency, the multimodal attention sink exhibits significant improvement over dense attention, with only a modest increase in time and memory costs compared to window attention and vanilla attention sink. These additional costs stem from retaining extra image tokens in the KV cache.', 'Quantitative results presented in Table', 'This work introduces SEED-Story, a pioneering approach that leverages a Multimodal Large Language Model to generate multimodal long stories with rich narrative text and contextually relevant images. We propose a multimodal attention sink mechanism to enable our model to generalize to generating long sequences in an efficient manner. We further present a high-quality dataset named StoryStream for training and benchmarking the task of multimodal story generation effectively. Our usage fully comply with the terms of use. 1) Personal Uses Permitted: My project is noncommercial and educational, which aligns with personal uses as outlined by PBS. we are not using the information for commercial purposes or exploiting it in a manner inconsistent with PBS rules. The use is strictly for educational and research purposes within an academic setting. 2) User\'s Obligation to Abide By Applicable Law: We will ensure all research activities comply with local laws, particularly those relating to copyright and intellectual property rights. Our use will not involve unauthorized reproduction, distribution, or exhibition that violates Intellectual Property Laws. All data are for research only. 3) Content of Information: We will responsibly use the "Curious George" materials, ensuring that all content used in our research is accurately cited and acknowledged. Any PBS content incorporated into your project will be clearly attributed to PBS.', '"Rabbids Invasion" is a French-American computer-animated TV series that breathes life into the zany antics of Ubisoft\'s popular Rabbids video game characters. Created by Jean-Louis Momus and featuring the voice of Damien Laquet, the show is a dynamic blend of humor and adventure tailored for a family audience. Since its debut on August 3, 2013, on France 3, the series has enjoyed multiple seasons and a global reach. The Rabbids are mischievous rabbit-like creatures whose escapades lead them into all sorts of unpredictable and hilarious situations, making "Rabbids Invasion" a delight for both kids and adults alike. Thanks to their release, we derive some subsets from the cartoon series Rabbids Invasion', 'The Land Before Time, an iconic animated film series created by Judy Freudberg and Tony Geiss and distributed by Universal Pictures, debuted in 1988 with significant contributions from Don Bluth, George Lucas, and Steven Spielberg. This franchise, consisting of an initial film followed by 13 sequels, a TV series, video games, and extensive merchandising, explores the adventures of five young dinosaurs who learn key life lessons about friendship and teamwork through their prehistoric trials. Despite the absence of the original creators in the sequels, the series has continued to captivate audiences, emphasizing themes of community and perseverance across its extensive narrative arc. Thanks to their release, we derive some subsets from their websites', 'Leveraging the data derived from "Curious George," "Rabbids Invasion," and "The Land Before Time," we have significantly advanced the capabilities of our story generation models. This progress has direct and impactful implications for children\'s education by enhancing their imaginative faculties and fostering a keen interest in learning. By integrating elements from these animated series into our models, we not only engage young minds but also deepen their affection for animated storytelling. Consequently, this not only meets but also amplifies educational objectives, such as improving literacy and cognitive skills through enjoyable and interactive content. The successful application of data from these beloved animations in our research exemplifies how academic pursuits can harmoniously blend with educational entertainment, ultimately delivering multifaceted benefits that extend well beyond conventional learning environments.', 'Lastly, we extend our profound appreciation to the creators and maintainers of "Curious George," "Rabbids Invasion," and "The Land Before Time," each a rich and vibrant resource that has significantly contributed to the scope and success of our research. The engaging narratives and characters from these series, especially the ever-curious George, the mischievous Rabbids, and the adventurous dinosaurs from The Land Before Time, have provided invaluable data that enhanced our narrative generation models. This project benefited immensely from the educational and entertaining content crafted with meticulous attention to detail, fostering imagination and learning in young audiences. We acknowledge the pivotal role that these animated series have played in advancing academic research aimed at educational technology. The commitment of the teams behind these beloved series to fostering curiosity and learning is both inspiring and exemplary. We are immensely grateful for the opportunity to incorporate such cherished resources into our scholarly work.', 'In this section, we present additional visualization comparison of our SEED-Story and other story visualization methods. SEED-Story shows better image consistency and higher qualit, as shown in Figure', 'In this section, we present our multimodal story generation results of our SEED-Story. It keeps produce story image and text with high quality. Please see Figure', 'We also showcase the visualization result of our model on Rabbids Invasion and The Land Before Time. Please see Figure', 'To evaluate the effectiveness of MM-interleaved and SEED-Story in multimodal story generation, we initiate an experiment where each model produces a story of five segments, based on a common starting image and text. The segment limit is set to five to accommodate the constraints of GPT-4V, which can handle a maximum of ten images per input session. In total, we generate 180 stories for assessment. For evaluation, we employ GPT-4 or GPT-4V to determine which model produces the better story in each case, based on the framework established in L-Eval', '"Please act as an impartial judge and evaluate the quality of the generation story contents provided by two AI assistants. Your job is to evaluate which assistant\'s generation is better. Your evaluation should consider {the style consistency of the story images / the engagement of the story / the coherence of the generated text and images}. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants.Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: "', ']" if assistant B is better, and "[[C]]" for a tie."', 'We also provide a prompt for directly estimating the performance of the generated results without comparing to others. The prompt we used is shown below. We present the direct estimation score is shown in Table', 'To showcase the capabilities of our multimodal generation model, we employ a video generation technique to animate the images. We then synchronize these moving images with audio to create a narrative video, which is available in our supplementary materials.', 'This project may potentially produce copyrighted content, particularly when used inappropriately or without adherence to existing intellectual property laws. To mitigate this risk, we will implement a rigorous compliance framework that respects the copyrights of third parties. This involves setting strict usage licenses that align with the legal standards dictated by our data sources. Our aim is to protect intellectual property rights while fostering innovation and ethical use of our technology. We also commit to educating users on the importance of respecting intellectual property rights when using our technology. This will be achieved through detailed user guidelines, training sessions, and readily available support to help users understand and navigate the complexities of copyright laws. By taking these measures, we aim not only to comply with legal standards but also to promote a culture of respect for intellectual property within our user community, thereby contributing positively to the broader digital ecosystem.', 'Lack of Realistic Data Experimentation: This limitation points to a potential gap in the validation of the SEED-Story model under practical, real-world conditions. Without experiments using realistic data, it\'s difficult to ascertain how the model would perform in scenarios that are not perfectly controlled or that deviate from the training conditions. This can be crucial, especially in applications like storytelling where the context and variability of real-world data play significant roles. A possible solution would be to incorporate a broader range of test conditions, including noisy data or data from "in-the-wild" storytelling scenarios, to evaluate the robustness and adaptability of the model.', "Training on a Non-Diverse Dataset: The second limitation is the restriction of the model's training to animation datasets which does not cover a large scale or diverse styles. This can severely limit the model's ability to generalize and produce outputs in styles that are not represented in the training data. This is particularly limiting in creative tasks such as storytelling where the ability to adapt to various artistic and narrative styles is crucial. To mitigate this, expanding the dataset to include a wider array of styles, genres, and visual aesthetics could be beneficial.", 'a) The man looked to the side with a surprised expression. The background appeared to be the interior of a rounded structure with a window. b) A small monkey looked up at a person wearing an ou<it. In the background, another person stood next to a large dinosaur skeleton inside what appeared to be a museum. c) A dinosaur skeleton stood inside a museum, with a woman in a coat gesturing towards it. In the background, a small figure waved enthusiasAcally from behind a glass window. d) An woman with short hair and a lab coat stood with hands on hips, smirking. The background showed a wall with two light switches. The man led George into a nearby building, where they found a cozy room with a wooden bed adorned with flowers. A lamp beside the bed cast a warm glow, making George feel right at home. Behind the door, George found a sleeping cat. He gently pulled the door closed, leaving the cat to sleep in peace.', 'The next morning, George woke up feeling cheerful. He sat up in bed, looking around the room with a smile, ready for another day of adventures.', 'George decided to take the cat for a walk. He held the cat by the collar, looking quite pleased with himself.', 'George held the cat in his arms, looking at the curious cat. He was enjoying this new experience of being a cat walkkeeper.', 'George decided to take the cat for a walk around the building. He held the cat in his arms, looking at the curious cat. They passed by a man in a coat, who smiled at the sight of them.', 'George decided to take the cat for another walk. He held the cat in his arms, looking at the wall, ready for another adventure. In a nearby park, a pigeon had found a new friend, George, the curious monkey. They were standing on a wooden ledge, peering over it. Nearby, a taller bird observed them with interest.', "Meanwhile, a gray pigeon had found a new perch on a building's edge. It overlooked a small balcony, where a man was enjoying a morning cup of coffee, oblivious to the pigeon's presence.", "Elsewhere, a blue-gray pigeon was pecking at seeds on the ground. A person nearby watched intently, their eyes following the bird's every move.", 'George, the small brown monkey, was standing next to a large bird on a ledge. The bird looked content, while George seemed curious. The background showed a building with windows, a perfect playground for a curious monkey.', 'Meanwhile, a cartoon bird was pecking at some food on the ground. A human hand reached towards the bird, offering a treat.', 'George was back on the ledge, this time looking puzzled. The background showed a building, perfect for his little puzzle.', 'In a playful mood, George mimicked a pigeon, pecking at a crumb-covered ledge. The pigeon watched him, amused. In the background, a building stood tall, perfect for their little playground.', "George was having a great time, leaning on the bird's perch with a smile. The background showed a building, perfect for a little monkey adventure.", 'George sat on a ledge, looking puzzled at a crumby area. The background showed a building, perfect for his little thought bubble.', 'Feeling adventurous, George stood on a city sidewalk, waving with a happy expression. The background showed colorful buildings and a clear sky, perfect for his little adventure.', 'George stood on a city sidewalk, waving with one hand. The background showed colorful buildings and a clear sky, perfect for his little adventure.', 'George stood on a city sidewalk, looking puzzled. The background showed colorful buildings and a street with a parked car, perfect for his little thought bubble.', 'Suddenly, George noticed a small object on the floor. He stood on the ledge, looking at it with curiosity. The background showed a building, perfect for his little investigations.', 'Suddenly, George noticed a small object on the ground. He stood on his two legs, reaching out to pick it up. Nearby, a pigeon rested on a ledge.', 'In summary, Our contributions are three-fold.', 'Quantitative evaluation of long story generation with various attention mechanisms. FID and CLIP scores are calculated by comparing ground truth images with generated images. Inference time and memory usage are calculated by generating 50 sequences multiple times for average.', 'For visual tokenization, we use Qwen-VL pre-trained ViT-G. We first resize the image to 448x448 images and then use ViT to produce its feature of length256 with 4096 dimension. (shape:', 'Instruction tuning data is formatted as follows: for each story, we sample a random length and compute losses on the last sequence (highlighted in red text). The sequence format is structured as:', 'For our language model (LLM), we utilize the LLAMA2-7B pre-trained model and finetune it using LoRA, supported by the peft library. The hyperparameter r is set to 6, and lora_alpha is set to 32. The modules optimized include the q_projection_layer, v_projection_layer, k_projection_layer, o_projection_layer, gate_projection_layer, down_projection_layer, and up_projection_layer. We employ a learning rate of 1 × 10 -4 to finetune this model on our dataset across approximately 6 epochs, utilizing 8 NVIDIA-A800 GPUs.', "In this stage we fully finetune the SD-XL model. The data format is as the same as instruction tuning, but we fix all MLLM params and optimize only the SD-XL. It takes the MLLM output and is asked to produce image correspond to the ground truth. The SD-XL model was trained using 4 NVIDIA-A800 GPUs. A learning rate of 1 × 10 -4 was chosen to facilitate gradual weight updates, ensuring stable convergence, while a weight decay of 0.03 was applied for regularization to prevent overfitting. Training was performed using mixed precision (bf16), which significantly reduced memory usage and accelerated the training process without compromising the model's accuracy. The model underwent three training epochs, balancing the learning of complex patterns against computational resource use, optimized for large-scale datasets and sophisticated model architectures.", "In this section, we present additional visualizations of attention maps. These maps are derived from various model runs, including varying data lengths, attention heads, and layers. The visualizations consistently reveal a pattern of attention focused on '0' tokens, punctuation, tokens adjacent to Begin-of-Image (BoI), and tokens adjacent to End-of-Image (EoI).", 'Curious George is an animated series featuring George, a curious monkey whose adventures teach preschoolers about math, science, and engineering. Guided by The Man with the Yellow Hat, George explores the world through problem-solving and experimentation, making it a delightful and educational experience for young viewers.', 'Curious George is released on PBS KIDS']