['Large language models (LLMs) have demonstrated impressive writing capabilities', 'We explore these challenges by focusing on how to generate Wikipedia-like articles from scratch. We decompose this problem into two tasks. The first is to conduct research to generate an outline, i.e., a list of multi-level sections, and collect a set of reference documents. The second uses the outline and the references to produce the full-length article. Such a task decomposition mirrors the human writing process which usually includes phases of pre-writing, drafting, and revising', 'As pre-trained language models inherently possess a wealth of knowledge, a direct approach is to rely on their parametric knowledge for generating outlines or even entire articles (Direct Gen). However, this approach is limited by a lack of details and hallucinations', 'Human learning theories', 'The design of STORM is based on two hypotheses: (1) diverse perspectives lead to varied questions; (2) formulating in-depth questions requires iterative research. Building upon these hypotheses, STORM employs a novel multi-stage approach. It first discovers diverse perspectives by retrieving and analyzing Wikipedia articles from similar topics and then personifies the LLM with specific perspectives for question asking (Figure', 'We evaluate STORM using our FreshWiki dataset ( §2.1) which curates recent, high-quality Wikipedia articles to avoid data leakage during pretraining.', 'We further invited a group of experienced Wikipedia editors for expert evaluation. The editors found STORM outperforms an outline-driven RAG baseline, especially regarding the breadth and organization of the articles. They also identified challenges for future research, including addressing cases where: (1) the bias on the Internet affects the generated articles; (2) LLMs fabricate connections between unrelated facts. These challenges present new frontiers to grounded writing systems.', 'Our main contributions include:', '• To evaluate the capacity of LLM systems at generating long-form grounded articles from scratch, and the pre-writing challenge in particular, we curate the FreshWiki dataset and establish evaluation criteria for both outline and final article quality.', '• We propose STORM, a novel system that automates the pre-writing stage. STORM researches the topic and creates an outline by using LLMs to ask incisive questions and retrieving trusted information from the Internet.', '• Both automatic and human evaluation demonstrate the effectiveness of our approach. Expert feedback further reveals new challenges in generating grounded long-form articles.', 'We study generating Wikipedia-like articles from scratch, placing emphasis on the pre-writing stage', 'Our setup emphasizes the capability of longform grounded writing systems to research and curate content. Specifically, given a topic t, the task is to find a set of references R and generate a full-length article S = s 1 s 2 ...s n , where each sentence s i cites a list of documents in R.', 'Creating a new Wikipedia-like article demands not only fluent writing but also good research skills. As modern LLMs are generally trained on Wikipedia text, we mitigate data leakage by explicitly seeking out recent Wikipedia articles that were created (or very heavily edited) after the training cutoff of the LLMs we test. Our process can be repeated at future dates when new LLMs emerge.', 'To apply our date criteria, we focus on the top 100 most-edited pages, based on edit counts, for each month from February 2022 to September 2023', 'A full-length article is hard to generate or evaluate', 'To evaluate the outline coverage, we introduce two metrics: heading soft recall and heading entity recall. These metrics compare the multi-level section headings of the human-written article, considered as ground truth, and those in O. Recognizing that an exact match between elements in these two sets of headings is unnecessary, we calculate the heading soft recall (Fränti and Mariescu-Istodor, 2023) using cosine similarity derived from Sentence-BERT', 'We present STORM to automate the pre-writing stage by researching a given topic via effective question asking ( §3.1, §3.2) and creating an outline ( §3.3). The outline will be extended to a fulllength article grounded on the collected references ( §3.4). Figure', 'Rohman (1965) defines pre-writing as the stage of discovery in the writing process. In parallel with stakeholder theory in business', 'Given the input topic t, STORM discovers different perspectives by surveying existing articles from similar topics and uses these perspectives to control the question asking process. Specifically, STORM prompts an LLM to generate a list of related topics and subsequently extracts the tables of contents from their corresponding Wikipedia articles, if such articles can be obtained through Wikipedia API 7  (Figure', 'The theory of questions and question asking', 'To ensure that the conversation history provides factual information, we use trusted sources from the Internet to ground the answer a i to each query q i . Since q i can be complicated, we first prompt the LLM to break down q i into a set of search queries (Figure', 'After thoroughly researching the topic through N + 1 simulated conversations, denoted as {C 0 , C 1 , ..., C N }, STORM creates an outline before the actual writing starts. To fully leverage the internal knowledge of LLMs, we first prompt the model to generate a draft outline O D given only the topic t (Figure', "Building upon the references R collected and the outline O developed during the pre-writing stage, the full-length article can be composed section by section. Since it is usually impossible to fit the entire R within the context window of the LLM, we use the section title and headings of its all-level subsections to retrieve relevant documents from R based on semantic similarity calculated from Sentence-BERT embeddings. With the relevant information at hand, the LLM is then prompted to generate the section with citations. Once all sections are generated, they are concatenated to form the full-length article. Since the sections are generated in parallel, we prompt the LLM with the concatenated article to delete repeated information to improve coherence. Furthermore, in alignment with Wikipedia's stylistic norms, the LLM is also utilized to synthesize a summary of the entire article, forming the lead section at the beginning.", 'STORM is capable of researching complicated topics and writing long articles from detailed outlines. However, in this controlled experiment, we limit the final output to at most 4000 tokens (roughly 3000 words). For a meaningful comparison, we randomly select 100 samples from the FreshWiki dataset (see §2.1) that have human-written articles not exceeding 3000 words.', 'As discussed in §2.2, we evaluate the outline quality to assess the pre-writing stage by calculating the heading soft recall and heading entity recall. A higher recall score signifies a more comprehensive outline relative to the human-written article.', 'To assess the full-length article quality, we adopt ROUGE scores', 'As prior works use different setups and do not use LLMs, they are hard to compare directly. Instead, we use the following three LLM-based baselines.', '1. Direct Gen, a baseline that directly prompts the LLM to generate an outline, which is then used to generate the full-length article.', '2. RAG, a retrieval-augmented generation baseline that searches with the topic and uses the searched results together with the topic t to generate an outline or the entire article.', '3. Outline-driven RAG (oRAG), which is identical to RAG in outline creation, but further searches additional information with section titles to generate the article section by section.', 'We build STORM with zero-shot prompting using the DSPy framework', 'in STORM are both set as 5. We use the chat model gpt-3.5-turbo for question asking and use gpt-3.5-turbo-instruct for other parts of STORM. We also experiment with using gpt-4 for drafting and refining the outline (Figure', 'For reported results, the simulated topic expert in STORM is grounded on the You.com search API 10 , although the proposed pipeline is compatible with other search engines. The ground truth Wikipedia article is excluded from the search results.', 'For final article generation, we only report the results using gpt-4 as gpt-3.5 is not faithful to sources when generating text with citations', 'We use outline coverage as a proxy to assess the prewriting stage (see §2.2). Table', 'We further evaluate the full-length article quality. As shown in Table', 'Although this work primarily focuses on the prewriting stage and does not optimize generating text with citations, we still examine the citation quality of articles produced by our approach. As reported in Table', 'As introduced in §3, STORM prompts LLMs to ask effective questions by discovering specific perspectives and simulating multi-turn conversations. We conduct the ablation study on outline creation by comparing STORM with two variants:', '(1) "STORM w/o Perspective", which omits perspective in the question generation prompt; (2) "STORM w/o Conversation", which prompts LLMs to generate a set number of questions altogether. To ensure a fair comparison, we control an equal total number of generated questions across all variants.', 'Table', 'To better understand the strengths and weaknesses of STORM, we conduct human evaluation by collaborating with 10 experienced Wikipedia editors who have made at least 500 edits on Wikipedia and have more than 1 year of experience. We randomly sample 20 topics from our dataset and evaluate the articles generated by our method and oRAG, the best baseline according to the automatic evaluation. Each pair of articles is assigned to 2 editors. We request editors to judge each article from the same five aspects defined in §4.2, but using a 1 to 7 scale for more fine-grained evaluation. While our automatic evaluation uses citation quality as a proxy to evaluate Verifiability, we stick to the Wikipedia standard of "verifiable with no original research" in human evaluation. Besides rating the articles, editors are asked to provide open-ended feedback and pairwise preference. After the evaluation finishes, they are further requested to compare an article produced by our method, which they have just reviewed, with its human-written counterpart, and report their perceived usefulness of STORM using a 1-5 Likert scale. More human evaluation details are included in Appendix D. Table', 'Retrieval-Augmented Generation (RAG) Augmenting language models (LMs) with retrieval at inference time is a typical way to leverage external knowledge stores', 'As a general framework, RAG is flexible in both the retrieval source and time. The retrieval sources can vary from domain databases', 'Question Asking in NLP Question asking capabilities in NLP systems have expanded across several fronts, including generating clarification questions to understand user intents', 'We propose STORM, an LLM-based writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch. We curate the FreshWiki dataset and establish evaluation criteria to study the generation of grounded longform articles. Experimental results demonstrate that the question asking mechanism in STORM improves both the outline and article quality. With the improved breadth and depth, STORM helps surface new challenges for grounded writing systems through expert evaluation. The experienced Wikipedia editors in our study unanimously agree that STORM is helpful for their pre-writing stage.', 'In this work, we explore generating Wikipedialike articles from scratch as a way to push the frontier of automatic expository writing and longform article generation. While our approach significantly outperforms baseline methods in both automatic and human evaluations, the quality of machine-written articles still lags behind wellrevised human-authored articles, specifically in aspects of neutrality and verifiability. Although STORM discovers different perspectives in researching the given topic, the collected information may still be biased towards dominant sources on the Internet and may contain promotional content. Moreover, the verifiability issues identified in this work go beyond factual hallucination, which highlights new challenges to grounded writing systems.', 'Another limitation of this work is that although we focus on the task of generating Wikipedia-like articles from scratch, our task setup is still simpli-fied to only consider the generation of free-form text. Human-authored high-quality Wikipedia articles usually contain structured data and multimodal information. We leave the exploration of generating multi-modal grounded articles for future work.', 'Different from the creative generation, grounded article generation may impact how people learn about topics or consume source information. All the studies and the evaluation in this work are designed to prevent the dissemination of misinformation by not publishing generated content online and implementing strict accuracy checks. We avoid any disruption to Wikipedia or related communities, as our system does not interact with live pages. Also, although we try to generate grounded articles, we believe there is no privacy issue related to this work as we only use information publicly available on the Internet.', 'The primary risk of our work is that the Wikipedia articles written by our system are grounded on information on the Internet which contains some biased or discriminative content on its own. Currently, our system relies on the search engine to retrieve information but does not include any post-processing module. We believe improving the retrieval module to have good coverage of different viewpoints and adding a content sifting module to the current system will be a critical next step to achieve better neutrality and balance in the generated articles.', 'Another limitation we see from an ethical point of view is that we only consider writing English Wikipedia articles in this work. Extending the current system to a multilingual setup is a meaningful direction for future work as more topics do not have Wikipedia pages in non-English languages.', 'As discussed in §2.1, we curate the FreshWiki dataset by collecting recent and high-quality English Wikipedia articles. We select the most-edited pages over a specific period rather than using creation dates as a cutoff because most of Wikipedia articles are "stubs" or are of low quality when they were created. For quality, we consider articles predicted to be of B-class quality or above. According to Wikipedia statistics', 'As LLMs can generate reasonably good outputs, we think it is important to use high-quality humanwritten articles as references for further research.', 'For experiments in this work, we randomly select 100 samples with human-written articles under 3000 words to have a meaningful comparison. Table', 'In §3, we introduce STORM, a framework that automates the pre-writing stage by discovering different perspectives, simulating information-seeking conversations, and creating a comprehensive outline. Algorithm 1 displays the skeleton of STORM.', 'We implement STORM with zero-shot prompting using the DSPy framework', 'We calculate the soft heading recall between the multi-level headings in the generated outline, considered as the prediction P , and those in the humanwritten article, considered as the ground truth G. The calculation is based on the soft recall definition in Fränti and Mariescu-Istodor (2023). Given a set A = {Ai} K i=1 , soft count of an item is defined as the inverse of the sum of its similarity to other items in the set:', 'where embed(•) in Equation (', '4', 'Please list the urls in separate lines .', '""" class GenQnPrompt ( dspy . Signature ):', '"""', 'You are an experienced Wikipedia writer and want to edit a specific page .', 'Besides your identity as a Wikipedia writer , you have a specific focus when researching the topic . Now , you are chatting with an expert to get information . Ask good questions to get more useful information .', 'When you have no more question to ask , say " Thank you so much for your help !" to end the conversation . Listing 1: Prompts used in STORM, corresponding to', 'class GenAnswerPrompt ( dspy . Signature ):', '2 """', 'You are an expert who can use information effectively . You are chatting with a Wikipedia writer who wants to write a Wikipedia page on topic you know . You have gathered the related information and will now use the information to form a response .', 'Make your response as informative as possible and make sure every sentence is supported by the gathered information .', '""" """', '16', 'Write an outline for a Wikipedia page .', '17', 'Here is the format of your writing : """', 'Improve an outline for a Wikipedia page . You already have a draft outline that covers the general information . Now you want to improve it based on the information learned from an information -seeking conversation to make it more comprehensive .', 'Here is the format of your writing : 30 1. Use "#" Title " to indicate section title , "##" Title " to indicate subsection title , "###" Title " to indicate subsubsection title , and so on . Listing 2: Prompts used in STORM (continue), corresponding to Line 24, 31, 32 in Algorithm 1.', 'of A is the sum of the counts of its individual items:', 'The soft heading recall is calculated as', 'where the cardinality of intersection is defined via the union as follows:', 'We use Prometheus', 'While Prometheus is best used with a score 5 reference answer, we find adding the reference will exceed the context length limit of the model. Since', 'We show the error distribution in Figure', 'We recruited 10 experienced Wikipedia editors to participate in our study by creating a research page on Meta-Wiki 17 and reaching out to active editors who have recently approved articles for Wikipedia. 18 Our participation group includes 3 editors with 1-5 years of experience, 4 with 6-10 years, and 3 with over 15 years of contribution. The study was approved by the Institutional Review Board of our institution and the participants signed the consent form through Qualtrics questionnaires before the study started.', 'To streamline the evaluation of grounded articles, we developed a web application, which features a side-by-side display of the article and its citation snippets, to gather ratings and open-ended feedback 15', '16 Following', 'for each article. Figure', 'We collected the pairwise preferences and the perceived usefulness of STORM via an online questionnaire. Specifically, for the perceived usefulness, we request editors to rate their agreement with statements "I think it can be specifically helpful for my pre-writing stage (e.g., collecting relevant sources, outlining, drafting).", "I think it will help me edit a Wikipedia article for a new topic", "I think it can be a potentially useful tool for the Wikipedia community" on a Likert scale of 1-5, corresponding to Strongly disagree, Somewhat disagree, Neither agree nor disagree, Somewhat agree, Strongly agree.', 'While articles produced by STORM are preferred by both automatic metrics and human evaluation, experienced editors still identified multiple problems with the machine-generated articles. We analyze the free-form comments and summarize the major issues in Table', "The primary issue raised is that the generated articles often contain emotional language and lack neutrality, primarily due to the source material. STORM currently retrieves grounding sources from the Internet which is not neutral and contains considerable promotional content on its own. Addressing this bias in the pre-writing stage represents a valuable direction for future research. Another major issue is the red herring fallacy or the over-association of unrelated facts. Addressing this challenge calls for high-level sensemaking rather than mere fact-level verification. Polling from America shouldn't be included and links to climate change shouldn't be made unless explicitly connected by the source. (comment on article Typhoon Hinnamnor)", "Sourcing seems mostly fine, though some aren't directly related", '(comment on article Gehraiyaan)', 'Here is a lengthy digression about KISS, not necessary because the article on the band should be linked to. (comment on article 2022 AFL Grand Final)', 'Missing important information 6', '"One study, conducted by Sinéad Griffin, a physicist at the Lawrence Berkeley National Laboratory, provided some analysis of LK-99\'s abilities using supercomputer simulations', "Although the earthquake's immediate aftermath and response are adequately covered, there could be more about the long-term socioeconomic impact and recovery processes.", '(comment on article 2022 West Java earthquake)', 'Improper handling of time-sensitive information 5', 'Words like "now" should be avoided in Wikipedia articles to prevent them from becoming dated and phrases such as, "as of December 2023" should be used instead.', '(comment on article Cyclone Batsirai) "as of December 13" doesn\'t specify a year, and is old information (comment on article 2022 West Java earthquake)', 'Section organization problem 5 too many subsections in the "Recovery and Rehabilitation" section (comment on article 2022 West Java earthquake)', 'I do not like how the article is organized, with too many headers cluttering the article, making it not as readable. Other than that, the AI did great work on the piece.', '(comment on article 2022 Crimean Bridge explosion)', 'Table', "His first major musical experience came from playing drums for Alanis Morissette's album, Jagged Little", 'Pill, and accompanying her on the subsequent tour[3]. This marked the beginning of his professional career in the music industry.', "Taylor Hawkins began his professional music career playing in Alanis Morissette's band during her 18-month world tour in support of the hit album 'Jagged Little Pill", 'and "You Learn" introduced him to the world of rock music and ultimately led to his meeting with Dave', 'Grohl', 'In 1997, Hawkins was asked by Grohl to join the Foo Fighters, an invitation that he readily accepted', "At the time, Grohl thought it was a long shot to recruit Hawkins given that Morissette was at the height of her career, but Hawkins' desire to be a part of a rock band compelled him to make the move", 'Taylor Hawkins was a profound drummer, with his musical style and influences spreading across a wide In his personal life, Hawkins had also struggled with drug use, which nearly claimed his life in a 2001 overdose[9]', 'Outside of his main role in the Foo Fighters, Hawkins also pursued various side projects including the Birds of Satan, NHC, and Chevy Metal. His motivation for such ventures was a constant drive to create and his love for music', 'Taylor Hawkins was known for his raw and authentic drumming style, described as "courageous, damaged', 'Taylor Hawkins also led a notable music career through his own side projects and collaborations', "Throughout his career, Taylor Hawkins collaborated with various prominent artists and bands. The Coattail Riders' albums notably featured appearances from luminaries such as Brian May and Roger Taylor of Queen, Chrissie Hynde, Nancy Wilson of Heart, Sex Pistol Steve Jones and James Gang's Joe Walsh", 'Hawkins also fronted another group, The Birds of Satan, which evolved from his heavy rock covers band, Chevy Metal', 'Despite his diverse musical engagements, Hawkins always maintained a close allegiance with the Foo Fighters, which remained the center of his music life', 'Taylor Hawkins, the esteemed drummer of the alt-rock band Foo Fighters, passed away suddenly on March 25, 2022, while on tour with his band in Bogotá, Colombia', "The news of Hawkins' sudden demise was announced on the morning of March 25th, 2022, which left the music world in shock", "As a result of Hawkins' untimely passing, the band canceled their ongoing South American tour", "In the wake of Hawkins' death, tributes from fans and colleagues alike poured in from around the world", 'Taylor, the drummer of Queen, who Hawkins credited with inspiring his own career behind the drum set', 'In heartfelt social media posts, Taylor described Hawkins as an "inspirational mentor" and a "kind', 'He was heavily influenced by his love for classic rock, as evidenced by his performances, where he covered songs from bands like', 'and unflinchingly authentic"', 'Our resources and code are released at https://github. com/stanford-oval/storm.', 'In practice, S also includes organizational elements such as section and subsection titles, which do not require citations.', 'Obtained fromhttps://wikimedia. org/api/rest_v1/metrics/edited-pages/ top-by-edits/en.wikipedia/all-editor-types/ content/{year}/{month}/all-days', 'https://www.mediawiki.org/wiki/ORES', 'https://en.wikipedia.org/wiki/Wikipedia: Stand-alone_lists', 'Since language models process and produce sequences, we can linearize O by adding "#" to indicate section titles, "##" to indicate subsection titles, etc.', 'https://en.wikipedia.org/wiki/Wikipedia: Reliable_sources', 'https://en.wikipedia.org/wiki/Wikipedia: Good_article_criteria', 'https://en.wikipedia.org/wiki/Wikipedia: Content_assessment', 'https://huggingface.co/sentence-transformers/ paraphrase-MiniLM-L6-v2', 'Do not include other information .', 'https://huggingface.co/kaist-ai/ prometheus-13b-v1.0', 'We thank', "Criteria Description Interest Level: How engaging and thought-provoking is the article? Score 1 Description Not engaging at all; no attempt to capture the reader's attention. Score 2 Description Fairly engaging with a basic narrative but lacking depth. Score 3 Description Moderately engaging with several interesting points. Score 4 Description Quite engaging with a well-structured narrative and noteworthy points that frequently capture and retain attention. Score 5 Description Exceptionally engaging throughout, with a compelling narrative that consistently stimulates interest.", 'Criteria Description Coherence and Organization: Is the article well-organized and logically structured? Score 1 Description Disorganized; lacks logical structure and coherence. Score 2 Description Fairly organized; a basic structure is present but not consistently followed. Score 3 Description Organized; a clear structure is mostly followed with some lapses in coherence. Score 4 Description Good organization; a clear structure with minor lapses in coherence. Score 5 Description Excellently organized; the article is logically structured with seamless transitions and a clear argument.', 'Criteria Description Relevance and Focus: Does the article stay on topic and maintain a clear focus? Score 1 Description Off-topic; the content does not align with the headline or core subject. Score 2 Description Somewhat on topic but with several digressions; the core subject is evident but not consistently adhered to. Score 3 Description Generally on topic, despite a few unrelated details. Score 4 Description Mostly on topic and focused; the narrative has a consistent relevance to the core subject with infrequent digressions.', 'Exceptionally focused and entirely on topic; the article is tightly centered on the subject, with every piece of information contributing to a comprehensive understanding of the topic.', "Criteria Description Broad Coverage: Does the article provide an in-depth exploration of the topic and have good coverage? Score 1 Description Severely lacking; offers little to no coverage of the topic's primary aspects, resulting in a very narrow perspective. Score 2 Description Partial coverage; includes some of the topic's main aspects but misses others, resulting in an incomplete portrayal. Score 3 Description Acceptable breadth; covers most main aspects, though it may stray into minor unnecessary details or overlook some relevant points. Score 4 Description Good coverage; achieves broad coverage of the topic, hitting on all major points with minimal extraneous information.", 'Score 5 Description Exemplary in breadth; delivers outstanding coverage, thoroughly detailing all crucial aspects of the topic without including irrelevant information.', 'Table', 'Improper Inferential Linking Lahaina, Hawaii Throughout its history, religion has remained the paramount aspect of Hawaiian life in Lahaina , permeating every daily activity and significant event[5].', '[5] "Religion, Beliefs & Spirituality" (The source discusses religion as part of Hawaiian life but does not mention Lahania .)', "Completed in June 2020 , the bridge serves as a major supply route for Russian forces in the region and is significant to Russia's claim over the disputed territory", '[2] "Crimean Bridge -Wikipedia" (The source says "The first scheduled passenger train crossed the bridge on 25 December 2019, while the bridge was opened for freight trains on 30 June 2020 ".)', 'Citing Irrelevant Sources LK-99', 'For example, comparisons have been drawn between the performance of LK-9 and the dynamic resolution capabilities of video games such as Battlefield 2042', '[22] "Battlefield 2042 PC performance guide: The best settings for a high frame rate" ( The source is irrelevant to LK-99. )', 'Being able to establish correspondences between pixels across different images of the same scene, denoted as image matching, constitutes a core component of all 3D vision applications, spanning mapping', 'In the past, matching methods have traditionally been cast into a three-steps pipeline consisting of first extracting sparse and repeatable keypoints, then describing them with locally invariant features, and finally pairing the discrete set of keypoints by comparing their distance in the feature space. This pipeline has several merits: keypoint detectors are precise under low-to-moderate illumination and viewpoint changes, and the sparsity of keypoints makes the problem computationally tractable, enabling very precise matching in milliseconds whenever the images are viewed under similar conditions. This explains the success and persistence of SIFT', 'Unfortunately, keypoint-based methods, by reducing matching to a bag-of-keypoint problem, discard the global geometric context of the correspondence task. This makes them especially prone to errors in situation with repetitive patterns or low-texture areas, which are in fact ill-posed for local descriptors. One way to remedy this is to introduce a global optimization strategy during the pairing step, typically leveraging some learned priors about matching, which SuperGlue and similar methods successfully implemented', 'Nevertheless, even a top-performing methods like LoFTR', 'In this paper, we point out that, while DUSt3R', 'To summarize, we claim three main contributions. First, we propose MASt3R, a 3D-aware matching approach building on the recently released DUSt3R framework. It outputs local feature maps that enable highly accurate and extremely robust matching. Second, we propose a coarse-to-fine matching scheme associated with a fast matching algorithm, enabling to work with high-resolution images. Third, MASt3R significantly outperform the state-of-the-art on several absolute and relative pose localization benchmarks.', 'Keypoint-based matching has been a cornerstone of computer vision. Matching is carried out in three distinct stages: keypoint detection, locally invariant description and nearest-neighbor search in descriptor space. Departing from the former handcrafted methods like SIFT', 'Dense matching. In contrast to keypoint-based approaches, semi-dense', 'Camera Pose estimation techniques vary widely, but the most successful strategies, for speed, accuracy and robustness trade-off, are fundamentally based on pixel matching', 'Grounding matching in 3D thus becomes a crucial necessity in these challenging conditions where classical 2D-based matching utterly falls short. Leveraging priors about the physical properties of the scene in order to improve accuracy or robustness has been widely explored in the past, but most previous works settle for leveraging epipolar constraints for semi-supervised learning of correspondences without any fundamental change', 'Given two images 𝐼 1 and 𝐼 2 , respectively captured by two cameras 𝐶 1 and 𝐶 2 with unknown parameters, we wish to recover a set of pixel correspondences {(𝑖, 𝑗)} where 𝑖, 𝑗 are pixels 𝑖 = (𝑢 𝑖 , 𝑣 𝑖 ), 𝑗 = (𝑢 𝑗 , 𝑣 𝑗 ) ∈ {1, . . . , 𝑊 }×{1, . . . , 𝐻}, 𝑊, 𝐻 being the respective width and height of the images. We assume they have the same resolution for the sake of simplicity, yet without loss of generality. The final network can handle pairs of variable aspect ratios.', 'Our approach, illustrated in fig.', 'DUSt3R', 'Both images are first encoded in a Siamese manner with a ViT', "Then, two intertwined decoders process these representations jointly, exchanging information via crossattention to 'understand' the spatial relationship between viewpoints and the global 3D geometry of the scene. The new representations augmented with this spatial information are denoted as 𝐻 1 and 𝐻 2 :", 'Finally, two prediction heads regress the final pointmaps and confidence maps from the concatenated representations output by the encoder and decoder:', 'Regression loss. DUSt3R is trained in a fully-supervised manner using a simple regression loss', 'where 𝑣 ∈ {1, 2} is the view and 𝑖 is a pixel for which the ground-truth 3D point X 𝑣,1 ∈ ℝ 3 is defined. In the original formulation, normalizing factors 𝑧, ẑ are introduced to make the reconstruction invariant to scale. These are simply defined as the mean distance of all valid 3D points to the origin.', 'Metric predictions. In this work, we note that scale invariance is not necessarily desirable, as some potential use-cases like map-free visual localization necessitates metric-scale predictions. Therefore, we modify the regression loss to ignore normalization for the predicted pointmaps when the ground-truth pointmaps are known to be metric. That is, we set 𝑧 := ẑ whenever ground-truth is metric, so that ℓ regr (𝑣, 𝑖) = ||𝑋 𝑣,1 𝑖 -X 𝑣,1 𝑖 ||/ẑ in this case. As in DUSt3R', '𝑖 ℓ regr (𝑣, 𝑖) -𝛼 log 𝐶 𝑣 𝑖 .', '(7)', 'To obtain reliable pixel correspondences from pointmaps, a standard solution is to look for reciprocal matches in some invariant feature space', 'Matching head. For these reasons, we propose to add a second head that outputs two dense feature maps 𝐷 1 and 𝐷 2 ∈ ℝ 𝐻 ×𝑊 ×𝑑 of dimensional 𝑑:', 'We implement the head as a simple 2-layers MLP interleaved with a non-linear GELU activation function', 'Matching objective. We wish to encourage each local descriptor from one image to match with at most a single descriptor from the other image that represents the same 3D point in the scene. To that aim, we leverage the infoNCE', 'with', 'Here, P 1 = {𝑖|(𝑖, 𝑗) ∈ M} and P 2 = { 𝑗|(𝑖, 𝑗) ∈ M} denote the subset of considered pixels in each image and 𝜏 is a temperature hyper-parameter. Note that this matching objective is essentially a cross-entropy classification loss: contrary to regression in eq. (', 'Given two predicted feature maps 𝐷 1 , 𝐷 2 ∈ ℝ 𝐻 ×𝑊 ×𝑑 , we aim to extract a set of reliable pixel correspondences, i.e. mutual nearest neighbors of each others:', 'Unfortunately, naive implementation of reciprocal matching has a high computational complexity of 𝑂(𝑊 2 𝐻 2 ), since every pixel from an image must be compared to every pixels in the other image. While optimizing the nearest-neighbor (NN) search is possible, e.g. using K-d trees', 'Fast matching. We therefore propose a faster approach based on sub-sampling. It is based on an iterated process that starts from an initial sparse set of 𝑘 pixels', ', typically sampled regularly on a grid in the first image 𝐼 1 . Each pixel is then mapped to its NN on 𝐼 2 , yielding 𝑉 1 , and the resulting pixels are mapped back again to 𝐼 1 in the same way:', 'The set of reciprocal matches (those which form a cycle, i.e. M 𝑡 𝑘 = {(𝑈 𝑡 𝑛 , 𝑉 𝑡 𝑛 ) | 𝑈 𝑡 𝑛 = 𝑈 𝑡+1 𝑛 }) are then collected. For the next iteration, pixels that already converged are filtered out, i.e. updating 𝑈 𝑡+1 := 𝑈 𝑡+1 \\ 𝑈 𝑡 . Likewise, starting from 𝑡 = 1 we also verify and filter 𝑉 𝑡+1 , comparing it with 𝑉 𝑡 in a similar fashion. As illustrated in fig.', 'Due to the quadratic complexity of attention w.r.t. the input image area (𝑊 × 𝐻), MASt3R only handles images of 512 pixels in their largest dimension. Larger images would require significantly more compute power to train, and ViTs do not generalize yet to larger test-time resolutions', 'Coarse-to-fine matching is a standard technique to preserve the benefit of matching high-resolution images with a lower-resolution algorithm', 'Correspondences obtained from each window pair are finally mapped back to the original image coordinates and concatenated, thus providing dense full-resolution matches.', 'We detail in section 4.', 'Training data . We train our network with a mixture of 14 datasets: Habitat', 'ScanNet++', 'Training. We base our model architecture on the public DUSt3R model', 'Correspondence sampling. To generate ground-truth correspondences necessary for the matching loss (eq. (', 'Fast nearest neighbors. For the fast reciprocal matching from section 3.3, we implement the nearest neighbor function NN (𝑥) from eq. (', 'Ablations on losses and matching modes. We report results on the validation set in table 1 for different variants of our approach: DUSt3R matching 3D points (I); MASt3R also matching 3D points (II) or local features (III, IV, V). For all methods, we compute the relative pose from the essential matrix', 'or from the depth directly output by MASt3R (V).', 'First, we note that all proposed methods significantly outperforms the DUSt3R baseline, probably because MASt3R is trained longer and with more data. All other things being equal, matching descriptors perform significantly better than matching 3D points (II versus IV). This confirms our initial analysis that regression is inherently unsuited to compute pixel correspondences, see section 3.2.', 'We also study the impact of training only with a single matching objective (L match from eq. (', 'Comparisons on the test set is reported in table', 'We also provide the results of direct regression with MASt3R, i.e. without matching, simply using PnP on the pointmap 𝑋 2,1 of the second image. These results are surprisingly on par with our matching-based variant, even though the ground-truth calibration of the reference camera is not used. As we show below, this does not hold true for other localization datasets, and computing the pose via matching (e.g. with PnP or essential matrix) with known intrinsics seems safer in general.', 'Qualitative results. We show in fig.', 'Datasets and protocol. Next, we evaluate for the task of relative pose estimation on the CO3Dv2', 'Baselines and metrics. As before, matches obtained with MASt3R are used to estimate Essential Matrices and relative pose. Please note that our predictions are always done pairwise, contrary to all other methods that leverage multiple views (at the exception of DUSt3R-PnP). We compare to recent data-driven approaches like RelPose', 'We also report results for more traditional SfM methods like PixSFM', 'Similar to', 'Results. As shown in table', 'Datasets. We then evaluate MASt3R for the task of absolute pose estimation on the Aachen Day-Night', 'Metrics. We report report the percentage of successfully localized images within three thresholds: (0.25m, 2°), (0.5m, 5°) and (5m, 10°) for Aachen and (0.25m, 10°), (0.5m, 10°), (1m, 10°) for InLoc.', 'Results are reported in Table', 'We finally perform MVS by triangulating the obtained matches. Note that the matching is performed in full resolution without prior knowledge of cameras, and the latter are only used to triangulate matches in groundtruth reference frame. We remove spurious 3D points via geometric consistency post-processing', 'Datasets and metrics. We evaluate our predictions on the DTU', 'Results. Data-driven approaches trained on this domain significantly outperform handcrafted ones, cutting the Chamfer error by half. To the best of our knowledge, we are the first to draw such conclusion in a zero-shot setting. MASt3R not only outperforms the DUSt3R baseline but also compete with the best methods, all without leveraging camera calibration nor poses for matching, neither having seen this camera setup before.', 'Table', 'RealEstate10K RRA@15 RTA@15 mAA', 'Colmap+SG', 'MVSNet', 'Grounding image matching in 3D with MASt3R significantly raised the bar on camera pose and localization tasks on many public benchmarks. We successfully improved DUSt3R with matching, getting the best of both worlds: enhanced robustness, while attaining and even surpassing what could be done with pixel matching alone. We introduced a fast reciprocal matcher and a coarse to fine approach for efficient processing, allowing users to balance between accuracy and speed. MASt3R is able to perform in few-view regimes (even in top1), that we believe will greatly increase versatility of localization.', 'We provide here additional qualitative results on the DTU', 'We show in fig.', 'Qualitative matching results. We show a few examples of matches fig.', 'We detail here the theoretical proofs of convergence of the Fast Reciprocal Matching algorithm presented in Sec.3.3 of the main paper. Contrary to the traditional bipartite graph matching formulation', '20) After this back-and-forth mapping, the reciprocal matches (i.e. those which form a cycle) are recovered and removed from 𝑈 𝑡+1 . The remaining "active" ones are mapped back to 𝐼 2 and reciprocity is checked again. We iterate this process for a few iterations. After enough iterations we discard any active sample remaining.', 'It is important to note that the NN algorithm we use is deterministic and consistently returns the same index in the case where multiple descriptors in the other image share the same minimal distance (or maximal similarity), although this is very unlikely since descriptors are real-valued. We show in orange the starting points of a convergence basin, i.e. nodes of a sub-graph for which the algorithm will converge towards the same cycle. For clarity, all edges of G were not drawn.', "Proof of Convergence. By design, Fast Reciprocal Matching (FRM) operates on the directed bipartite graph G of nearest neighbors between 𝐼 1 and 𝐼 2 . G contains oriented edges E. All nodes, i.e. pixels, belong to G since we add an edge for each pixel's nearest neighbor, but note that all pixels cannot reach all other pixels. For example, two reciprocal pixels in 𝐼 1 and 𝐼 2 are only connected to each other and to no other pixels. This means G is composed of possibly multiple disjoint sub-graphs G 𝑖 , 1 ≤ 𝑖 ≤ 𝐻𝑊 with directed edges E 𝑖 (see fig.", 'Proof. This is a rather trivial fact, since we build G s.t. only one edge exits each node. If one were to follow the path of a sub-graph G 𝑖 , once a node that belongs to a cycle is reached, no edge can exit the cycle, for the only exiting edge is already part of the cycle. A second cycle (or more) thus cannot exist in G 𝑖 . □ Lemma B. 𝑣 the similarity score of an edge between two nodes 𝑢 and 𝑣, (𝑢, 𝑣) ∈ E 𝑖 . Because edges are nearest neighbors, we note that 𝑠(𝑎, 𝑏) ≤ 𝑠(𝑐, 𝑏). This trivially stems from the fact that if 𝑠(𝑐, 𝑏) < 𝑠(𝑎, 𝑏) then the nearest neighbor of 𝑏 would no longer be 𝑐 but at least 𝑎. Expanding this property to the path along G 𝑖 it follows that: 𝑠(𝑎, 𝑏) ≤ 𝑠(𝑐, 𝑏) ≤ 𝑠(𝑐, 𝑑) ≤ 𝑠(𝑒, 𝑑)...', 'Meaning that the similarity score monotonously increases as we walk along the graph. There is a finite number of nodes in G 𝑖 so this sequence reaches the upper-bound similarity value 𝑠(𝑢, 𝑣). Because 𝑠(𝑢, 𝑣) is the maximal similarity in G 𝑖 , this ensures that 𝑁 𝑁 2 (𝐷 1 𝑢 ) = 𝑣 and 𝑁 𝑁 1 (𝐷 2 𝑣 ) = 𝑢 forming a cycle of at least two nodes. This means there is always a cycle in G 𝑖 , between the maximal similarity pair. Following proposition B.1, we can conclude that there is no other cycle in G 𝑖 and that each starting point is thus guaranteed to lead towards the root via a single path, forming an arborescence with a cycle at its root. □', 'Note that the root cycle can be of more than two nodes if more than one greatest similarity of eq. (', '𝑠(𝑎, 𝑏) = 𝑠(𝑐, 𝑏) = 𝑠(𝑐, 𝑑) = 𝑠(𝑎, 𝑑). This is extremely unlikely with real-valued distance and we consider it is negligible.', 'This follows naturally from the above: we did not make any assumption about the starting point of this walk nor about the sub-graph it belongs to. For any starting point in the graph, i.e. for all initial pixels 𝑈, the FRM algorithm will by design follow the sub-graph of nearest neighbors that will ultimately lead to the root cycle, which is by definition a reciprocal match.', 'We illustrate this behavior in fig.', 'Note that it is possible to artificially build a graph that maximizes the number of NN queries thus impacting the computational efficiency, but these are very unlikely in practice as seen in Figure', 'The number of active samples, e.g. samples that did not Proof. This fact comes trivially from the 𝑘 sparse initial samples 𝑈. As explained before, G is composed of at most 𝐻𝑊 sub-graphs G 𝑖 . Because we initialize the algorithm with 𝑘 ≪ 𝐻𝑊 seeds, these can at most span 𝑘 sub-graphs each leading to a single reciprocal match. Due to the potential presence of convergence basins, as seen in fig.', 'As observed in Figure', 'In order to demonstrate the effect of basin-biased sam-pling, we propose to compute the full correspondence set M (eq. (', 'In this section, we showcase the important benefits of the coarse-to-fine strategy. We compare it to coarseonly matching, that simply computes correspondences on input images down-scaled to the resolution of the network.', 'Visual localization on Aachen Day-Night', 'In our experiments, we set the confidence loss weight 𝛼 = 0.2 as in', 'mann. Map-free visual relocalization: Metric pose relative to a single image. In ECCV, 2022. 2, 3, 6, 7, 8, 11 [6] Chow Ashley, Trulls Eduard, HCL-Jevster, Yi', 'In this appendix, we first present additional qualitative examples on various tasks in appendix A, followed by a proof of convergence of the fast reciprocal matching algorithm and an in-depth study of the related performance gains in appendix B. We finally show an ablative study concerning the impact of coarse-to-fine matching in appendix C.', 'ViT. The Vision Transformer (ViT)', 'Mamba. Since the introduction of Mamba, a number of efforts have been proposed to leverage its capability for vision applications. Specifically, Vim', 'EfficientVMamba', 'In addition, VMamba', 'In this section, we introduce MambaVision which is our proposed novel architecture with SOTA performance on ImageNet-1K dataset. As illustrated in Fig.', 'In this section, we first revisit the preliminaries of Mamba and SSMs. We then present the micro design of the architecture in stages 3 and 4 and discuss MambaVision formulation in more details.', 'Mamba is an extension of Structured state space sequence models (S4) which is capable of transforming a 1D continuous input', 'Discretization The continuous parameters A, B and C in the above formulation are further converted into discrete parameters for better computational efficiency', 'The Eq. 2 can then be expressed with discrete parameters as', "In addition, for an input sequence with size T , a global convolution with kernel K can be applied for computing the output of Eq. 4 as in the following  Selectivity Mamba further extends the S4 formulation by introducing a selection mechanism which allows for input-dependant sequence processing. This allows the model's parameters B, C and ∆ to be adjusted dynamically according to the inputs and filter out irrelevant information. Further discretization details are provided in", 'Assuming an input X ∈ R T ×C with sequence length T with embedding dimension C, the output of layer n in stages 3 and 4 can be computed as in', 'Norm and Mixer denote the choices of layer normalization and token mixing blocks, respectively. Without loss of generality, Layer Normalization is used for Norm. Given N layers, the first N 2 layers employ MambaVision mixer blocks while the remaining N 2 layers employ self-attention. We describe the details of each mixer in the following.', 'As shown in Fig.', 'Linear(C in , C out )(•) denotes a linear layer with C in and C out as input and output embedding dimensions, Scan is the selective scan operation as in', 'In general, our proposed modification leads to richer feature representations, better generalization, and improved performance on computer vision tasks. We have also experimentally validated the effectiveness of each of our design choices in Sec.', 'Self-attention We use a generic multihead self-attention mechanism in accordance to', 'Q, K, V denote query, key and value respectively and d h is the number of attention heads. Without loss of generality, the attention formulation which can be computed in a windowed manner similar to previous efforts', 'Image classification experiments are conducted on ImageNet-1K dataset', 'To evaluate the performance of downstream tasks, we used our pre-trained models as backbones for object detection and instance segmentation and semantic segmentation tasks and used the MS COCO dataset', 'In Table', 'trends in comparison to Mamba-based models. Specifically, MambaVision-B (84.2%) outperforms VMamba-B (83.9%) despite having considerably higher image throughput. We would also like to note that although our main design goal has been to the accuracy and throughput tradeoff, the MambaVision model variants have much lower FLOPs when compared to similarly-sized counterparts. For instance, MambaVision-B has 56% less GFLOPs than MaxViT-B.', 'symmetric branch (i.e. conv2). As expected, this formulation achieves a suboptimal performance with a Top-1 accuracy of 80.9% (-1.8%), box AP and mask AP of 44.8 (-1.6) and 40.2 (-1.6) and mIoU of 44.2% (-1.4). We then replace the causal conv of SSM branch (i.e. conv1) with a regular conv layer and observe improvements in terms of all metrics due to this change. Furthermore, we add conv2 layer but use the same gating mechanism of Mamba instead of concatenation. This change improves the performance and leads to a Top-1 accuracy of 81.3%, box AP and mask AP of 45.3 and 41.0 and mIoU of 45.7%. Finally, adding concatenation considerably improves the performance in all metrics by +1.0%, +1.1, +0.8 and +0.9 in terms of ImageNet Top-1, box AP and mask AP for MS COCO and mIoU for ADE20K dataset. Hence, this validates our hypothesis that concatenating the outputs from both branches (i.e. SSM and non-SSM) leads to learning richer feature representation and enhancing global context understanding. Table', 'Hybrid Pattern In this section, we comprehensively study the effect of different hybrid integration patterns for self-attention and MambaVision token mixers. For all experiments, the architecture follows MambaVision-T layout with and we keep the models iso-parameter for fair comparison. The patterns are utilized for stages 3 and 4 with hybrid functionality. We begin our study by using a random pattern and achieve a suboptimal Top-1 accuracy of 81.3%. This confirms our previous intuition that simply using self-attention without a specific pattern may not be effective. We then utilize the self-attention blocks in the first N/2 layers for each stage, with N representing the total number of stage layers, and observe an improvement of +0.2% (81.5%) in Top-1 accuracy. However, using a mixed layer pattern of self-attention/MambaVision mixers blocks slightly reduces the accuracy by -0.1% (81.4%). Conversely, reversing the order of mixed layers by using MambaVision/self-attention improves the performance and archives a Top-1 accuracy of (81.6%). We then utilize self-attention blocks in only the last N/4 layers of each stage and observe an immediate improvement of +0.3% (81.9%) in the accuracy. This validates our hypothesis that using self-attention blocks in the final layers of each stage is an effective design. However, its representation learning capability needs to be tuned with respect to MambaVision layers. Increasing the number of self-attention blocks to N/2 last layers of each stage achieves the best performance of 82.3%.', 'In this work, we introduced MambaVision which is the first Mamba-Transformer hybrid backbone specifically tailored for vision applications. We proposed re-design of Mamba formulation to enhance global context representation learning capability and presented a comprehensive study of hybrid design integration patterns. MambaVision achieves a new SOTA Pareto front in terms of Top-1 accuracy and image throughput, surpassing Transformer and Mamba-based models by a significant margin. We hope these findings could be the foundation for a new class of hybrid vision models.']